{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cddd.inference import InferenceModel\n",
    "from cddd.preprocessing import preprocess_smiles\n",
    "from scipy.stats import norm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import AllChem as Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = InferenceModel('/home/DeepLearning/cddd/default_model/', \n",
    "                                 use_gpu=False, beam_width=10, num_top=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = inference_model.seq_to_emb\n",
    "decoder = inference_model.emb_to_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/Development/GAN_testing/A2AR'\n",
    "data_file = 'A2AR.csv'\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(data_dir, data_file)\n",
    "init_data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>label</th>\n",
       "      <th>CMPD_CHEMBLID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>CCCCC(=O)N1CCN(c2nc(N)n3nc(-c4ccco4)nc3n2)CC1</td>\n",
       "      <td>1</td>\n",
       "      <td>CHEMBL1927435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3556</th>\n",
       "      <td>Nc1nc(-c2cccc(Cl)c2)nc2sc(CN3CCOCC3)cc12</td>\n",
       "      <td>1</td>\n",
       "      <td>CHEMBL3222105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>Cn1c2ccccc2n2c(=O)c(-c3ccco3)nnc12</td>\n",
       "      <td>0</td>\n",
       "      <td>CHEMBL7098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>CC(c1cc2ccccc2s1)N(O)C(N)=O</td>\n",
       "      <td>0</td>\n",
       "      <td>CHEMBL93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(C(N)=O)...</td>\n",
       "      <td>1</td>\n",
       "      <td>CHEMBL201791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SMILES  label  CMPD_CHEMBLID\n",
       "627       CCCCC(=O)N1CCN(c2nc(N)n3nc(-c4ccco4)nc3n2)CC1      1  CHEMBL1927435\n",
       "3556           Nc1nc(-c2cccc(Cl)c2)nc2sc(CN3CCOCC3)cc12      1  CHEMBL3222105\n",
       "3141                 Cn1c2ccccc2n2c(=O)c(-c3ccco3)nnc12      0     CHEMBL7098\n",
       "420                         CC(c1cc2ccccc2s1)N(O)C(N)=O      0       CHEMBL93\n",
       "912   CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(C(N)=O)...      1   CHEMBL201791"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoder(init_data.SMILES.values)\n",
    "ys = init_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X) == len(ys))\n",
    "X_file = os.path.join(data_dir, 'X_act.npy')\n",
    "Y_file = os.path.join(data_dir, 'Y_act.npy')\n",
    "np.save(X_file, X)\n",
    "np.save(Y_file, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN using pytorch\n",
    "#### https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "b1 = 0.0 # 0.5\n",
    "b2 = 0.9 #0.999\n",
    "batch_size = 32\n",
    "epochs = 10000\n",
    "latent_dim =512\n",
    "input_dim = 32\n",
    "# WGAN-GP\n",
    "n_critic = 5   # number of training steps for discriminator per iter\n",
    "acgan_scale = 1\n",
    "acgan_scale_G = 1\n",
    "# Loss weight for gradient penalty , default 10\n",
    "lambda_gp = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        if m.weight is not None:\n",
    "            init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.adv_layer = nn.Linear(64, 1)\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(64, num_classes), nn.Softmax()) #nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, molvec):\n",
    "        # Concatenate label and mol to produce input\n",
    "#         dis_input = torch.cat((molvec, labels), -1)\n",
    "        out = self.block(molvec)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "        return validity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep close to Bayer cddd paper dim=1024 for the input\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, input_dim)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(2048, latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        # Concatenate label and mol to produce input\n",
    "        gen_input = torch.mul(self.label_emb(labels), z)#torch.cat((labels, z), -1)\n",
    "        return self.model(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (block): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (4): Dropout(p=0.4)\n",
       "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (7): Dropout(p=0.4)\n",
       "  )\n",
       "  (adv_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (aux_layer): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (1): Softmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize G, D\n",
    "gen = Generator()\n",
    "dis = Discriminator()\n",
    "if torch.cuda.is_available():\n",
    "    gen.cuda()\n",
    "    dis.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "gen.apply(weights_init)\n",
    "dis.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(gen.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(dis.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "class LabeledMolvecs(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_file, Y_file, data_dir, transform=None):\n",
    "        self.X = np.load(os.path.join(data_dir, X_file))\n",
    "        self.Y = np.load(os.path.join(data_dir, Y_file))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index, :], self.Y[index])  #.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    LabeledMolvecs('X_act.npy', 'Y_act.npy', data_dir),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates, _ = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/envs/cddd/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10000] [Batch 0/145] [D loss: 2.438668] [G loss: 0.339236] [WD: -0.474179] [D acc: 46.875000]\n",
      "[Epoch 0/10000] [Batch 5/145] [D loss: 2.071541] [G loss: 0.137721] [WD: -0.271429] [D acc: 57.812500]\n",
      "[Epoch 0/10000] [Batch 10/145] [D loss: 1.616674] [G loss: -0.034660] [WD: -0.171434] [D acc: 65.625000]\n",
      "[Epoch 0/10000] [Batch 15/145] [D loss: 1.756667] [G loss: 0.145568] [WD: -0.434483] [D acc: 39.062500]\n",
      "[Epoch 0/10000] [Batch 20/145] [D loss: 1.456950] [G loss: 0.113352] [WD: -0.453810] [D acc: 60.937500]\n",
      "[Epoch 0/10000] [Batch 25/145] [D loss: 1.319772] [G loss: 0.065557] [WD: -0.642230] [D acc: 54.687500]\n",
      "[Epoch 0/10000] [Batch 30/145] [D loss: 0.944933] [G loss: 0.044022] [WD: -0.732060] [D acc: 57.812500]\n",
      "[Epoch 0/10000] [Batch 35/145] [D loss: 0.773891] [G loss: -0.050153] [WD: -0.948444] [D acc: 45.312500]\n",
      "[Epoch 0/10000] [Batch 40/145] [D loss: 0.819515] [G loss: -0.091972] [WD: -0.886656] [D acc: 48.437500]\n",
      "[Epoch 0/10000] [Batch 45/145] [D loss: 0.955783] [G loss: 0.182360] [WD: -0.980459] [D acc: 53.125000]\n",
      "[Epoch 0/10000] [Batch 50/145] [D loss: -0.111017] [G loss: -0.102897] [WD: -1.678918] [D acc: 50.000000]\n",
      "[Epoch 0/10000] [Batch 55/145] [D loss: 0.223246] [G loss: -0.042127] [WD: -1.575710] [D acc: 56.250000]\n",
      "[Epoch 0/10000] [Batch 60/145] [D loss: -0.152837] [G loss: -0.084701] [WD: -1.792685] [D acc: 45.312500]\n",
      "[Epoch 0/10000] [Batch 65/145] [D loss: 0.206118] [G loss: 0.000059] [WD: -1.607386] [D acc: 57.812500]\n",
      "[Epoch 0/10000] [Batch 70/145] [D loss: 0.384471] [G loss: 0.123949] [WD: -1.660805] [D acc: 57.812500]\n",
      "[Epoch 0/10000] [Batch 75/145] [D loss: 0.006753] [G loss: 0.015239] [WD: -1.727451] [D acc: 50.000000]\n",
      "[Epoch 0/10000] [Batch 80/145] [D loss: 0.181931] [G loss: -0.207506] [WD: -1.505438] [D acc: 65.625000]\n",
      "[Epoch 0/10000] [Batch 85/145] [D loss: 0.067678] [G loss: -0.189487] [WD: -1.593369] [D acc: 64.062500]\n",
      "[Epoch 0/10000] [Batch 90/145] [D loss: -0.731452] [G loss: -0.216677] [WD: -2.357024] [D acc: 53.125000]\n",
      "[Epoch 0/10000] [Batch 95/145] [D loss: 0.216052] [G loss: -0.380620] [WD: -1.846739] [D acc: 40.625000]\n",
      "[Epoch 0/10000] [Batch 100/145] [D loss: -0.089012] [G loss: -0.480629] [WD: -1.658922] [D acc: 45.312500]\n",
      "[Epoch 0/10000] [Batch 105/145] [D loss: -0.233382] [G loss: -0.354640] [WD: -1.809966] [D acc: 59.375000]\n",
      "[Epoch 0/10000] [Batch 110/145] [D loss: -0.353545] [G loss: -0.508912] [WD: -2.185751] [D acc: 50.000000]\n",
      "[Epoch 0/10000] [Batch 115/145] [D loss: 0.134051] [G loss: -0.372016] [WD: -1.501351] [D acc: 54.687500]\n",
      "[Epoch 0/10000] [Batch 120/145] [D loss: -0.105101] [G loss: -0.473129] [WD: -1.909937] [D acc: 56.250000]\n",
      "[Epoch 0/10000] [Batch 125/145] [D loss: -0.150278] [G loss: -0.711800] [WD: -1.557485] [D acc: 59.375000]\n",
      "[Epoch 0/10000] [Batch 130/145] [D loss: 0.043850] [G loss: -0.539518] [WD: -1.613526] [D acc: 42.187500]\n",
      "[Epoch 0/10000] [Batch 135/145] [D loss: 0.541957] [G loss: -0.959729] [WD: -1.160429] [D acc: 50.000000]\n",
      "[Epoch 0/10000] [Batch 140/145] [D loss: 0.299957] [G loss: -0.812094] [WD: -1.132165] [D acc: 57.812500]\n",
      "[Epoch 1/10000] [Batch 0/145] [D loss: 0.575788] [G loss: -0.776785] [WD: -0.804955] [D acc: 62.500000]\n",
      "[Epoch 1/10000] [Batch 5/145] [D loss: -0.271817] [G loss: -0.733323] [WD: -1.539918] [D acc: 50.000000]\n",
      "[Epoch 1/10000] [Batch 10/145] [D loss: 0.990852] [G loss: -1.091546] [WD: -0.542766] [D acc: 46.875000]\n",
      "[Epoch 1/10000] [Batch 15/145] [D loss: 0.539793] [G loss: -0.995390] [WD: -0.963805] [D acc: 48.437500]\n",
      "[Epoch 1/10000] [Batch 20/145] [D loss: 0.092603] [G loss: -0.932396] [WD: -1.196429] [D acc: 48.437500]\n",
      "[Epoch 1/10000] [Batch 25/145] [D loss: 0.618824] [G loss: -1.091728] [WD: -0.509833] [D acc: 62.500000]\n",
      "[Epoch 1/10000] [Batch 30/145] [D loss: 0.508286] [G loss: -0.999076] [WD: -0.692902] [D acc: 59.375000]\n",
      "[Epoch 1/10000] [Batch 35/145] [D loss: 0.970560] [G loss: -1.211971] [WD: -0.388292] [D acc: 50.000000]\n",
      "[Epoch 1/10000] [Batch 40/145] [D loss: 0.599578] [G loss: -0.830704] [WD: -0.447602] [D acc: 62.500000]\n",
      "[Epoch 1/10000] [Batch 45/145] [D loss: 0.906798] [G loss: -1.053453] [WD: -0.086384] [D acc: 50.000000]\n",
      "[Epoch 1/10000] [Batch 50/145] [D loss: 0.542070] [G loss: -1.220535] [WD: -0.386212] [D acc: 57.812500]\n",
      "[Epoch 1/10000] [Batch 55/145] [D loss: 1.008372] [G loss: -1.271414] [WD: 0.140363] [D acc: 73.437500]\n",
      "[Epoch 1/10000] [Batch 60/145] [D loss: 0.549115] [G loss: -1.111053] [WD: -0.374048] [D acc: 59.375000]\n",
      "[Epoch 1/10000] [Batch 65/145] [D loss: 0.248373] [G loss: -0.581421] [WD: -0.862784] [D acc: 57.812500]\n",
      "[Epoch 1/10000] [Batch 70/145] [D loss: 0.746129] [G loss: -0.718271] [WD: -0.290872] [D acc: 64.062500]\n",
      "[Epoch 1/10000] [Batch 75/145] [D loss: 0.469610] [G loss: -0.795965] [WD: -0.441542] [D acc: 59.375000]\n",
      "[Epoch 1/10000] [Batch 80/145] [D loss: 0.933458] [G loss: -1.176071] [WD: -0.001886] [D acc: 53.125000]\n",
      "[Epoch 1/10000] [Batch 85/145] [D loss: 0.247892] [G loss: -0.974440] [WD: -0.699786] [D acc: 57.812500]\n",
      "[Epoch 1/10000] [Batch 90/145] [D loss: 0.700013] [G loss: -1.059813] [WD: -0.323050] [D acc: 51.562500]\n",
      "[Epoch 1/10000] [Batch 95/145] [D loss: 0.989631] [G loss: -0.990913] [WD: 0.017397] [D acc: 56.250000]\n",
      "[Epoch 1/10000] [Batch 100/145] [D loss: 0.820130] [G loss: -0.721114] [WD: -0.287777] [D acc: 43.750000]\n",
      "[Epoch 1/10000] [Batch 105/145] [D loss: 0.538081] [G loss: -1.074415] [WD: -0.221696] [D acc: 73.437500]\n",
      "[Epoch 1/10000] [Batch 110/145] [D loss: 0.947160] [G loss: -0.905225] [WD: 0.037408] [D acc: 57.812500]\n",
      "[Epoch 1/10000] [Batch 115/145] [D loss: 0.401245] [G loss: -0.989324] [WD: -0.467209] [D acc: 56.250000]\n",
      "[Epoch 1/10000] [Batch 120/145] [D loss: 0.401239] [G loss: -0.542011] [WD: -0.479390] [D acc: 60.937500]\n",
      "[Epoch 1/10000] [Batch 125/145] [D loss: 0.810908] [G loss: -0.687407] [WD: -0.145268] [D acc: 57.812500]\n",
      "[Epoch 1/10000] [Batch 130/145] [D loss: 0.336955] [G loss: -0.772706] [WD: -0.577160] [D acc: 56.250000]\n",
      "[Epoch 1/10000] [Batch 135/145] [D loss: 0.856561] [G loss: -0.496370] [WD: 0.098907] [D acc: 73.437500]\n",
      "[Epoch 1/10000] [Batch 140/145] [D loss: 0.825222] [G loss: -0.366586] [WD: -0.150915] [D acc: 62.500000]\n",
      "[Epoch 2/10000] [Batch 0/145] [D loss: 0.529030] [G loss: -0.469729] [WD: -0.264257] [D acc: 51.562500]\n",
      "[Epoch 2/10000] [Batch 5/145] [D loss: 0.324011] [G loss: -0.389311] [WD: -0.590927] [D acc: 59.375000]\n",
      "[Epoch 2/10000] [Batch 10/145] [D loss: 0.534399] [G loss: -0.736030] [WD: -0.334717] [D acc: 57.812500]\n",
      "[Epoch 2/10000] [Batch 15/145] [D loss: 0.810813] [G loss: -0.497155] [WD: -0.035692] [D acc: 62.500000]\n",
      "[Epoch 2/10000] [Batch 20/145] [D loss: 0.507255] [G loss: -0.469736] [WD: -0.385132] [D acc: 56.250000]\n",
      "[Epoch 2/10000] [Batch 25/145] [D loss: 0.279444] [G loss: -0.664294] [WD: -0.539926] [D acc: 51.562500]\n",
      "[Epoch 2/10000] [Batch 30/145] [D loss: 0.362621] [G loss: -0.548039] [WD: -0.506173] [D acc: 59.375000]\n",
      "[Epoch 2/10000] [Batch 35/145] [D loss: 0.640298] [G loss: -0.500345] [WD: -0.141443] [D acc: 54.687500]\n",
      "[Epoch 2/10000] [Batch 40/145] [D loss: 0.406070] [G loss: -0.458118] [WD: -0.456890] [D acc: 50.000000]\n",
      "[Epoch 2/10000] [Batch 45/145] [D loss: 0.701823] [G loss: -0.502589] [WD: -0.145582] [D acc: 57.812500]\n",
      "[Epoch 2/10000] [Batch 50/145] [D loss: 0.190934] [G loss: -0.589905] [WD: -0.631180] [D acc: 59.375000]\n",
      "[Epoch 2/10000] [Batch 55/145] [D loss: 0.354487] [G loss: -0.713529] [WD: -0.588335] [D acc: 62.500000]\n",
      "[Epoch 2/10000] [Batch 60/145] [D loss: 0.727510] [G loss: -0.607707] [WD: -0.113298] [D acc: 60.937500]\n",
      "[Epoch 2/10000] [Batch 65/145] [D loss: 0.987204] [G loss: -0.619901] [WD: -0.006401] [D acc: 59.375000]\n",
      "[Epoch 2/10000] [Batch 70/145] [D loss: 0.397374] [G loss: -0.606638] [WD: -0.436116] [D acc: 56.250000]\n",
      "[Epoch 2/10000] [Batch 75/145] [D loss: 0.730365] [G loss: -0.589753] [WD: -0.134227] [D acc: 60.937500]\n",
      "[Epoch 2/10000] [Batch 80/145] [D loss: 0.682059] [G loss: -0.430427] [WD: -0.210533] [D acc: 56.250000]\n",
      "[Epoch 2/10000] [Batch 85/145] [D loss: 0.685326] [G loss: -0.598639] [WD: -0.131982] [D acc: 54.687500]\n",
      "[Epoch 2/10000] [Batch 90/145] [D loss: 0.429101] [G loss: -0.449923] [WD: -0.348921] [D acc: 62.500000]\n",
      "[Epoch 2/10000] [Batch 95/145] [D loss: 0.722166] [G loss: -0.156915] [WD: -0.161697] [D acc: 62.500000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9997/10000] [Batch 55/145] [D loss: -0.150480] [G loss: -0.091930] [WD: -0.857152] [D acc: 92.187500]\n",
      "[Epoch 9997/10000] [Batch 60/145] [D loss: -0.228761] [G loss: -0.145526] [WD: -1.051074] [D acc: 81.250000]\n",
      "[Epoch 9997/10000] [Batch 65/145] [D loss: -0.220584] [G loss: -0.142666] [WD: -0.943862] [D acc: 85.937500]\n",
      "[Epoch 9997/10000] [Batch 70/145] [D loss: -0.125907] [G loss: -0.216884] [WD: -0.740148] [D acc: 90.625000]\n",
      "[Epoch 9997/10000] [Batch 75/145] [D loss: -0.286333] [G loss: -0.350579] [WD: -0.864278] [D acc: 89.062500]\n",
      "[Epoch 9997/10000] [Batch 80/145] [D loss: -0.333137] [G loss: -0.022999] [WD: -1.064658] [D acc: 93.750000]\n",
      "[Epoch 9997/10000] [Batch 85/145] [D loss: -0.213069] [G loss: -0.053483] [WD: -0.783357] [D acc: 87.500000]\n",
      "[Epoch 9997/10000] [Batch 90/145] [D loss: -0.304944] [G loss: -0.234313] [WD: -0.957269] [D acc: 82.812500]\n",
      "[Epoch 9997/10000] [Batch 95/145] [D loss: -0.258049] [G loss: 0.053753] [WD: -0.895224] [D acc: 87.500000]\n",
      "[Epoch 9997/10000] [Batch 100/145] [D loss: -0.212525] [G loss: 0.077064] [WD: -0.945917] [D acc: 82.812500]\n",
      "[Epoch 9997/10000] [Batch 105/145] [D loss: -0.424975] [G loss: -0.207362] [WD: -1.055087] [D acc: 93.750000]\n",
      "[Epoch 9997/10000] [Batch 110/145] [D loss: -0.519136] [G loss: 0.035330] [WD: -1.028837] [D acc: 92.187500]\n",
      "[Epoch 9997/10000] [Batch 115/145] [D loss: -0.500947] [G loss: -0.188156] [WD: -1.052711] [D acc: 85.937500]\n",
      "[Epoch 9997/10000] [Batch 120/145] [D loss: -0.466632] [G loss: 0.006141] [WD: -1.010293] [D acc: 82.812500]\n",
      "[Epoch 9997/10000] [Batch 125/145] [D loss: -0.428078] [G loss: -0.129590] [WD: -1.045009] [D acc: 82.812500]\n",
      "[Epoch 9997/10000] [Batch 130/145] [D loss: -0.575102] [G loss: 0.061566] [WD: -1.195247] [D acc: 92.187500]\n",
      "[Epoch 9997/10000] [Batch 135/145] [D loss: -0.256057] [G loss: 0.167808] [WD: -0.829942] [D acc: 89.062500]\n",
      "[Epoch 9997/10000] [Batch 140/145] [D loss: -0.399666] [G loss: -0.139505] [WD: -1.127946] [D acc: 84.375000]\n",
      "[Epoch 9998/10000] [Batch 0/145] [D loss: -0.333252] [G loss: 0.064140] [WD: -0.900187] [D acc: 92.187500]\n",
      "[Epoch 9998/10000] [Batch 5/145] [D loss: -0.297336] [G loss: 0.142817] [WD: -0.895419] [D acc: 87.500000]\n",
      "[Epoch 9998/10000] [Batch 10/145] [D loss: -0.122872] [G loss: -0.127353] [WD: -0.772445] [D acc: 82.812500]\n",
      "[Epoch 9998/10000] [Batch 15/145] [D loss: -0.552635] [G loss: -0.436150] [WD: -1.229062] [D acc: 89.062500]\n",
      "[Epoch 9998/10000] [Batch 20/145] [D loss: -0.232115] [G loss: -0.293213] [WD: -1.031878] [D acc: 89.062500]\n",
      "[Epoch 9998/10000] [Batch 25/145] [D loss: -0.155540] [G loss: -0.434229] [WD: -0.778840] [D acc: 79.687500]\n",
      "[Epoch 9998/10000] [Batch 30/145] [D loss: -0.186415] [G loss: -0.419379] [WD: -0.756888] [D acc: 84.375000]\n",
      "[Epoch 9998/10000] [Batch 35/145] [D loss: -0.687782] [G loss: -0.098643] [WD: -1.360273] [D acc: 85.937500]\n",
      "[Epoch 9998/10000] [Batch 40/145] [D loss: -0.182001] [G loss: -0.054044] [WD: -0.908493] [D acc: 93.750000]\n",
      "[Epoch 9998/10000] [Batch 45/145] [D loss: -0.280687] [G loss: -0.154375] [WD: -0.955411] [D acc: 93.750000]\n",
      "[Epoch 9998/10000] [Batch 50/145] [D loss: -0.500052] [G loss: -0.135123] [WD: -1.043374] [D acc: 90.625000]\n",
      "[Epoch 9998/10000] [Batch 55/145] [D loss: -0.364111] [G loss: -0.015139] [WD: -0.987617] [D acc: 87.500000]\n",
      "[Epoch 9998/10000] [Batch 60/145] [D loss: -0.565444] [G loss: -0.036871] [WD: -1.274878] [D acc: 89.062500]\n",
      "[Epoch 9998/10000] [Batch 65/145] [D loss: -0.063188] [G loss: -0.131470] [WD: -0.945960] [D acc: 78.125000]\n",
      "[Epoch 9998/10000] [Batch 70/145] [D loss: -0.174529] [G loss: -0.294260] [WD: -0.974450] [D acc: 90.625000]\n",
      "[Epoch 9998/10000] [Batch 75/145] [D loss: -0.291919] [G loss: -0.223683] [WD: -1.009033] [D acc: 87.500000]\n",
      "[Epoch 9998/10000] [Batch 80/145] [D loss: -0.509437] [G loss: -0.256821] [WD: -1.082436] [D acc: 92.187500]\n",
      "[Epoch 9998/10000] [Batch 85/145] [D loss: -0.519186] [G loss: -0.186534] [WD: -1.172028] [D acc: 90.625000]\n",
      "[Epoch 9998/10000] [Batch 90/145] [D loss: -0.379191] [G loss: -0.164373] [WD: -1.082886] [D acc: 84.375000]\n",
      "[Epoch 9998/10000] [Batch 95/145] [D loss: -0.294993] [G loss: 0.055375] [WD: -0.950812] [D acc: 87.500000]\n",
      "[Epoch 9998/10000] [Batch 100/145] [D loss: -0.402919] [G loss: -0.131497] [WD: -1.012519] [D acc: 89.062500]\n",
      "[Epoch 9998/10000] [Batch 105/145] [D loss: -0.357170] [G loss: -0.317312] [WD: -0.997273] [D acc: 89.062500]\n",
      "[Epoch 9998/10000] [Batch 110/145] [D loss: -0.355765] [G loss: -0.147920] [WD: -1.096166] [D acc: 81.250000]\n",
      "[Epoch 9998/10000] [Batch 115/145] [D loss: -0.399924] [G loss: -0.106515] [WD: -1.036412] [D acc: 90.625000]\n",
      "[Epoch 9998/10000] [Batch 120/145] [D loss: -0.648956] [G loss: 0.130594] [WD: -1.314794] [D acc: 87.500000]\n",
      "[Epoch 9998/10000] [Batch 125/145] [D loss: -0.204900] [G loss: -0.243934] [WD: -0.846797] [D acc: 90.625000]\n",
      "[Epoch 9998/10000] [Batch 130/145] [D loss: -0.418607] [G loss: -0.121376] [WD: -0.994875] [D acc: 84.375000]\n",
      "[Epoch 9998/10000] [Batch 135/145] [D loss: -0.691908] [G loss: -0.064034] [WD: -1.222411] [D acc: 90.625000]\n",
      "[Epoch 9998/10000] [Batch 140/145] [D loss: -0.340999] [G loss: 0.039246] [WD: -0.978751] [D acc: 89.062500]\n",
      "[Epoch 9999/10000] [Batch 0/145] [D loss: -0.442567] [G loss: -0.223221] [WD: -1.119640] [D acc: 79.687500]\n",
      "[Epoch 9999/10000] [Batch 5/145] [D loss: -0.287357] [G loss: -0.403254] [WD: -1.159193] [D acc: 81.250000]\n",
      "[Epoch 9999/10000] [Batch 10/145] [D loss: -0.168825] [G loss: -0.039006] [WD: -1.023242] [D acc: 87.500000]\n",
      "[Epoch 9999/10000] [Batch 15/145] [D loss: -0.465793] [G loss: -0.160544] [WD: -1.091949] [D acc: 89.062500]\n",
      "[Epoch 9999/10000] [Batch 20/145] [D loss: -0.451985] [G loss: -0.056578] [WD: -1.072155] [D acc: 85.937500]\n",
      "[Epoch 9999/10000] [Batch 25/145] [D loss: -0.163484] [G loss: -0.381140] [WD: -0.849205] [D acc: 87.500000]\n",
      "[Epoch 9999/10000] [Batch 30/145] [D loss: -0.290875] [G loss: 0.197964] [WD: -0.940012] [D acc: 90.625000]\n",
      "[Epoch 9999/10000] [Batch 35/145] [D loss: -0.341543] [G loss: -0.138248] [WD: -0.900884] [D acc: 87.500000]\n",
      "[Epoch 9999/10000] [Batch 40/145] [D loss: -0.494815] [G loss: -0.348449] [WD: -1.227100] [D acc: 84.375000]\n",
      "[Epoch 9999/10000] [Batch 45/145] [D loss: -0.273949] [G loss: 0.130484] [WD: -1.007627] [D acc: 84.375000]\n",
      "[Epoch 9999/10000] [Batch 50/145] [D loss: -0.164238] [G loss: -0.146105] [WD: -0.716601] [D acc: 84.375000]\n",
      "[Epoch 9999/10000] [Batch 55/145] [D loss: -0.166103] [G loss: -0.048067] [WD: -0.697620] [D acc: 85.937500]\n",
      "[Epoch 9999/10000] [Batch 60/145] [D loss: -0.449364] [G loss: -0.118084] [WD: -1.041829] [D acc: 87.500000]\n",
      "[Epoch 9999/10000] [Batch 65/145] [D loss: -0.551738] [G loss: 0.000133] [WD: -1.218304] [D acc: 78.125000]\n",
      "[Epoch 9999/10000] [Batch 70/145] [D loss: -0.245092] [G loss: -0.100794] [WD: -0.837916] [D acc: 82.812500]\n",
      "[Epoch 9999/10000] [Batch 75/145] [D loss: -0.399100] [G loss: -0.197013] [WD: -1.191622] [D acc: 87.500000]\n",
      "[Epoch 9999/10000] [Batch 80/145] [D loss: -0.296792] [G loss: 0.062815] [WD: -0.901539] [D acc: 84.375000]\n",
      "[Epoch 9999/10000] [Batch 85/145] [D loss: -0.256145] [G loss: -0.173772] [WD: -0.890110] [D acc: 82.812500]\n",
      "[Epoch 9999/10000] [Batch 90/145] [D loss: -0.583367] [G loss: 0.112317] [WD: -1.370631] [D acc: 92.187500]\n",
      "[Epoch 9999/10000] [Batch 95/145] [D loss: -0.344461] [G loss: 0.114017] [WD: -0.999897] [D acc: 81.250000]\n",
      "[Epoch 9999/10000] [Batch 100/145] [D loss: -0.086013] [G loss: -0.055970] [WD: -0.747536] [D acc: 81.250000]\n",
      "[Epoch 9999/10000] [Batch 105/145] [D loss: -0.531197] [G loss: 0.298038] [WD: -1.275797] [D acc: 84.375000]\n",
      "[Epoch 9999/10000] [Batch 110/145] [D loss: -0.560995] [G loss: 0.132331] [WD: -1.073764] [D acc: 85.937500]\n",
      "[Epoch 9999/10000] [Batch 115/145] [D loss: -0.555177] [G loss: 0.420209] [WD: -1.205645] [D acc: 87.500000]\n",
      "[Epoch 9999/10000] [Batch 120/145] [D loss: -0.479599] [G loss: -0.010876] [WD: -1.040404] [D acc: 90.625000]\n",
      "[Epoch 9999/10000] [Batch 125/145] [D loss: -0.291326] [G loss: 0.040174] [WD: -0.898065] [D acc: 90.625000]\n",
      "[Epoch 9999/10000] [Batch 130/145] [D loss: -0.496429] [G loss: -0.305425] [WD: -1.056346] [D acc: 95.312500]\n",
      "[Epoch 9999/10000] [Batch 135/145] [D loss: -0.705634] [G loss: -0.356858] [WD: -1.260601] [D acc: 93.750000]\n",
      "[Epoch 9999/10000] [Batch 140/145] [D loss: -0.438492] [G loss: -0.243211] [WD: -1.086366] [D acc: 93.750000]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# TRAINING\n",
    "##########\n",
    "\n",
    "batches_done = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (molvecs, labels) in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        real_mols = Variable(molvecs.type(Tensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "        \n",
    "        #####################\n",
    "        # Train Discriminator\n",
    "        #####################\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (molvecs.shape[0], input_dim))))\n",
    "        f_labels = np.random.randint(0, num_classes, molvecs.shape[0])\n",
    "        fake_labels = Variable(LongTensor(f_labels))\n",
    "        \n",
    "        # Generate molecules\n",
    "        fake_mols = gen(z, fake_labels)\n",
    "        \n",
    "        # Real mols\n",
    "        real_validity, real_aux = dis(real_mols)\n",
    "        real_err_D = torch.mean(real_validity)\n",
    "        real_aux_err_D = auxiliary_loss(real_aux, labels)#.mean()\n",
    "        \n",
    "        # Fake mols\n",
    "        fake_validity, fake_aux = dis(fake_mols)\n",
    "        fake_err_D = torch.mean(fake_validity)\n",
    "        \n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(dis, real_mols.data, fake_mols.data)\n",
    "        # Adversarial loss\n",
    "        cost_D = -real_err_D + fake_err_D + lambda_gp * gradient_penalty\n",
    "        acgan_D = real_aux_err_D\n",
    "        \n",
    "        d_loss = cost_D + acgan_scale*acgan_D\n",
    "        \n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), fake_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        #################\n",
    "        # Train Generator\n",
    "        #################\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Train the generator every n_critic steps\n",
    "        if i % n_critic == 0:\n",
    "            # Sample noise and labels as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (molvecs.shape[0], input_dim))))\n",
    "            f_labels = np.random.randint(0, num_classes, molvecs.shape[0])\n",
    "            fake_labels = Variable(LongTensor(f_labels))\n",
    "            \n",
    "            # Generate molecules\n",
    "            fake_mols = gen(z, fake_labels)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake mols\n",
    "            fake_validity, pred_label = dis(fake_mols)\n",
    "#             f_labels = Variable(LongTensor(f_labels))\n",
    "            aux_loss = auxiliary_loss(pred_label, fake_labels)#.mean()\n",
    "            gen_cost = -torch.mean(fake_validity)\n",
    "            \n",
    "            g_loss = acgan_scale_G*aux_loss + gen_cost\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        \n",
    "            ##############################\n",
    "        \n",
    "            print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [WD: %f] [D acc: %f]\"\n",
    "            % (epoch, epochs, i, len(dataloader), d_loss.item(), g_loss.item(), \n",
    "               -real_err_D.item() + fake_err_D.item(), 100*d_acc)\n",
    "            )\n",
    "        \n",
    "            batches_done += n_critic\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/envs/cddd/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/anaconda3/envs/cddd/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(gen, os.path.join(data_dir, 'generator.pt'))\n",
    "torch.save(dis, os.path.join(data_dir, 'discriminator.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "def sample(gen, dis, batch_size, labels):\n",
    "    # set model in eval mode\n",
    "    gen.eval()\n",
    "    \n",
    "    def mol_from_xsmiles(smis):\n",
    "        if isinstance(smis, str):\n",
    "            return Chem.MolFromSmiles(smis)\n",
    "        for smi in smis:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is not None:\n",
    "                return mol\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, input_dim))))\n",
    "        labels = Variable(LongTensor(labels))\n",
    "    \n",
    "    # create a sample\n",
    "    x = gen(z, labels)\n",
    "#     __, D_labels = dis(x)\n",
    "    \n",
    "    x = x.cpu().detach().numpy()\n",
    "#     D_labels = D_labels.cpu().detach().numpy()\n",
    "#     print(np.argmax(D_labels, axis=1))\n",
    "    Y = decoder(x)\n",
    "    mols = [mol_from_xsmiles(y) for y in Y]\n",
    "    return mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/DeepLearning/cddd/default_model/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 1 2 3 9 10 11 12 20 21\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 20 21 27 28 33\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 20 21 27 28 33\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 4 5 6 7 8 20 21 27 28\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 3 4 5 12 19\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 15 22 23\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 15 22 23\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:38:22] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 15 22 23\n",
      "RDKit ERROR: \n"
     ]
    }
   ],
   "source": [
    "g_mols = sample(gen, dis, batch_size, np.ones(batch_size, dtype=int)*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QSAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "print(sklearn.__version__)\n",
    "\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import DataStructs, Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morganfp(mol, bits=4096, radius=3):\n",
    "    if mol is None: return\n",
    "    vec = np.ndarray((1, bits), dtype=int)\n",
    "    fp = Chem.GetMorganFingerprintAsBitVect(mol, radius, nBits=bits)\n",
    "    DataStructs.ConvertToNumpyArray(fp, vec)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(data_dir, 'A2AR.jbl')\n",
    "model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsar_vec = []\n",
    "good_mols=[]\n",
    "idx = []\n",
    "for i, m in enumerate(g_mols):\n",
    "    fp = morganfp(m)\n",
    "    if fp is not None:\n",
    "        qsar_vec.append(fp)\n",
    "        good_mols.append(m)\n",
    "    else:\n",
    "        idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(qsar_vec)#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.Draw.MolsToGridImage(g_mols, molsPerRow=2, maxMols=200, subImgSize=(600, 600),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O=C(Nc1ccc(-c2ccncc2)c(-c2ccccc2F)n1)C1CC1 1\n",
      "Cn1cc2c(nc(NC(=O)Cc3ccc(C(F)(F)F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc(-c2ccco2)nc2sc(CN3CCSCC3)cc12 1\n",
      "Nc1nc(-c2ccccc2)nc2sc(Cc3ccccc3)cc12 1\n",
      "Ratio of new: 0.87\n",
      "Mean activity 0.80\n"
     ]
    }
   ],
   "source": [
    "## But how many are there in the training set?\n",
    "good_smiles = list(Chem.MolToSmiles(x) for x in good_mols)\n",
    "count = 0\n",
    "new = 0\n",
    "for i, smi in enumerate(good_smiles):\n",
    "    count += 1\n",
    "    if smi in init_data.SMILES.tolist():\n",
    "        print(smi, preds[i])\n",
    "    else:\n",
    "        new += 1\n",
    "\n",
    "print('Ratio of new: {:.2f}'.format(new / count))\n",
    "mean_act = np.mean(preds)\n",
    "print('Mean activity {:.2f}'.format(mean_act))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tougher test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000 // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(mols):\n",
    "    smi = [Chem.MolToSmiles(x) for x in mols if x is not None and Chem.MolToSmiles(x) is not None]\n",
    "    smi = list(set(smi))\n",
    "    ret = [Chem.MolFromSmiles(x) for x in smi if Chem.MolFromSmiles(x) is not None]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mols_tough = []\n",
    "for i in range(n_samples):\n",
    "    g_mols_tough.append(sample(gen, dis, batch_size, np.ones(batch_size, dtype=int)*LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mols_tough = list(reduce(lambda x,y: x+y, g_mols_tough))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9984"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_mols_tough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mols = unique(g_mols_tough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsar_vec_tough = []\n",
    "good_mols_tough=[]\n",
    "idx = []\n",
    "for i, m in enumerate(unique_mols):\n",
    "    fp = morganfp(m)\n",
    "    if fp is not None:\n",
    "        qsar_vec_tough.append(fp)\n",
    "        good_mols_tough.append(m)\n",
    "    else:\n",
    "        idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_tough = model.predict(qsar_vec_tough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63467139420032925"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds_tough == 1) / len(preds_tough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7897"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nc1nc(-c2ccccc2)nc2cn(Cc3ccccc3)nc12 1\n",
      "CCC(=O)Nc1cc(-c2ccco2)nc(-c2ccco2)n1 1\n",
      "Nc1nc(-c2ccco2)c2cnn(Cc3ccccc3)c2n1 1\n",
      "Nc1nc(-c2ccccc2)cc(-c2ccccc2)n1 0\n",
      "Nc1nc(C(=O)NCc2cccc3cccnc23)c2ccccc2n1 1\n",
      "COc1ccc(Cn2cnc3c(-c4ccco4)nc(N)nc32)cc1 1\n",
      "CCn1cc2c(nc(NC(=O)Nc3ccc(F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc(-c2ccc(Br)o2)nc2sc(CN3CCCC3)cc12 1\n",
      "Cn1cc2c(nc(NC3CCCC3)n3nc(-c4ccco4)nc23)n1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)NCc4ccccc4)cc3)nc2n(CCC)c1=O 1\n",
      "Cn1cc2c(nc(NC(=O)Nc3ccc(C(F)(F)F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc2c(cnn2CCc2ccc(OCc3ccccc3)cc2)c2nc(-c3ccco3)nn12 1\n",
      "Nc1nc(-c2ccco2)c2cnn(Cc3ccccc3[N+](=O)[O-])c2n1 1\n",
      "Nc1nc(C(=O)NCc2ccco2)cc(-c2ccco2)n1 1\n",
      "O=C(Nc1ccc(-c2ccncc2F)c(-c2ccco2)n1)C1CC1 1\n",
      "Cc1nc(-c2sc(NC(=O)c3ccccc3)nc2-c2ccccc2)no1 1\n",
      "Cc1ccc(-c2cc(-c3ccccc3)nc(N)c2C#N)cc1 1\n",
      "CCCC(=O)Nc1cc(-c2ccccc2)nc(-c2ccccc2)n1 1\n",
      "Cc1cccc(-c2cc(C(=O)NCc3ccccn3)nc(N)n2)c1 1\n",
      "Nc1nc(-c2ccco2)c2ncn(Cc3cccc(Cl)c3)c2n1 1\n",
      "O=C(Nc1cnc(-c2ccncc2)c(-c2ccco2)n1)C1CC1 1\n",
      "Cn1c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCc5ccccc5C4)cc3)cc2n(C)c1=O 0\n",
      "N#Cc1c(-c2ccco2)cc(-c2ccccc2)nc1N 1\n",
      "Nc1nc(-c2ccco2)c2ncn(Cc3ccccc3F)c2n1 1\n",
      "Nc1cc(C(=O)N2CCCC2)cc2nc(-c3ccc(Br)o3)nn12 1\n",
      "CCn1cc2c(nc(NC(=O)Cc3cccc4ccccc34)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc2cn(CCc3ccccc3)cc2c2nc(-c3ccco3)nn12 1\n",
      "Cn1cc2c(nc(NC(=O)Nc3ccc(F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc(-n2cccn2)nc(N2CCC2)c1Br 1\n",
      "CCCc1nc2nc[nH]c2c2nc(-c3ccccc3)nn12 1\n",
      "COc1cccc2c1nc(N)n1nc(CN3CCN(c4ccccc4)CC3C)nc21 1\n",
      "CCn1nnc(-c2sc(NC(=O)Cc3ccccc3)nc2-c2ccccc2)n1 0\n",
      "COCC1CCCN1c1cc(NC(C)=O)nc(-n2nc(C)cc2C)n1 1\n",
      "O=C(Nc1cnc(-c2ccncc2)c(-c2ccccn2)n1)C1CC1 1\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2cccc(F)c2)n1)C1CC1 1\n",
      "Nc1nc2c(cnn2CCCc2ccc(OCc3ccccc3)cc2)c2nc(-c3ccco3)nn12 1\n",
      "Nc1nc(C(=O)NCc2ccccc2)cc(-c2ccco2)n1 1\n",
      "Cn1c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCN(c5ccccc5)CC4)cc3)cc2n(C)c1=O 1\n",
      "CCc1nc(-c2cccs2)c2sccc2n1 1\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3cccs3)c2n1 1\n",
      "Nc1nnc(-c2ccccc2)c(-c2ccccc2)n1 1\n",
      "CC(=O)Nc1cc(-c2cccc(C)c2)nc(-n2nc(C)cc2C)n1 1\n",
      "COc1ccc(CC(=O)Nc2cc(-c3nccs3)nc(-c3ccc(C)o3)n2)cc1 1\n",
      "CCCN(C)C(=O)c1cc(N)n2nc(-c3ccc(Br)o3)nc2c1 1\n",
      "Nc1nc(-c2ccccc2)nc2cn(-c3ccccc3)nc12 0\n",
      "Cc1ccccc1CNC(=O)c1cc(-c2ccco2)nc(N)n1 1\n",
      "Cn1cc2c(nc(NC(=O)Cc3ccccc3)n3nc(-c4ccccc4)nc23)n1 0\n",
      "O=C(O)c1cnc(NC2CCCC2)n2nc(-c3ccco3)nc12 0\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3ccccc3N)c2n1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(OC)cc4)cc3)cc2n(CCC)c1=O 1\n",
      "Nc1nc(C(=O)NCc2ccccn2)cc(-c2ccco2)n1 1\n",
      "COc1ccccc1-c1cc(C(=O)NCc2ccccn2)nc(N)n1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCN(c5ccccc5)CC4)cc3)cc2n(CCC)c1=O 1\n",
      "Cc1ccc(-n2nc3c(N)nc4ccccc4n3c2=O)cc1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(C(F)(F)F)cc4)cc3)nc2n(CCC)c1=O 1\n",
      "Nc1nc(-c2ccccc2)nc2sc(Cc3ccccc3)cc12 1\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2ccccc2F)n1)C1CC1 1\n",
      "Nc1nc(-c2ccco2)nc2sc(CN3CC=CC3)cc12 1\n",
      "Cc1ccc(CNC(=O)c2cc(-c3ccco3)nc(N)n2)cc1 1\n",
      "COc1cccc(-c2cc(NC(C)=O)nc(-n3nc(C)cc3C)n2)c1 1\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3ccccc3)c2n1 1\n",
      "Cn1cc2c(nc(N)n3nc(-c4ccccc4)nc23)n1 1\n",
      "Nc1nc2nn(CCc3ccccc3)cc2c2nc(-c3ccco3)nn12 1\n",
      "N#Cc1c(-c2ccco2)cc(-c2ccco2)nc1N 1\n",
      "Cc1ccc(-c2cc(C(=O)NCc3ccccn3)nc(N)n2)o1 1\n",
      "CCOC(=O)c1cnc(N(C)C)n2nc(-c3ccco3)nc12 0\n",
      "Nc1nc(-c2ccccn2)nc2sc(Cc3ccccc3)cc12 0\n",
      "Nc1nc2c(cnn2CCCc2ccc(OCCc3ccccc3)cc2)c2nc(-c3ccco3)nn12 1\n",
      "CC(=O)Nc1cc(-c2cccc(C#N)c2)nc(-n2nc(C)cc2C)n1 1\n",
      "Cc1cccc(CNC(=O)c2cc(-c3ccco3)nc(N)n2)c1 1\n",
      "Cn1c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCC(c5ccccc5)CC4)cc3)cc2n(C)c1=O 0\n",
      "Cc1cc(C)n(-c2cc(NC(=O)CN3CCN(C)CC3)nc(-c3ccc(C)o3)n2)n1 1\n",
      "Nc1nc(-c2ccco2)nc2sc(CN3CCSCC3)cc12 1\n",
      "Nc1nc2nn(CCCc3ccccc3)cc2c2nc(-c3ccco3)nn12 1\n",
      "Nc1nc(-c2ccccc2)c2c(n1)-c1cc(NCCN3CCOCC3)ccc1C2=O 1\n",
      "Nc1nc(NC2CCCCC2)nc2nc(-c3ccco3)nn12 1\n",
      "CCC(=O)Nc1cc(-c2ccccc2)nc(-c2ccccc2)n1 1\n",
      "CC(=O)Nc1cc(-c2cc(C)ccc2C)nc(-n2nc(C)cc2C)n1 1\n",
      "Cc1nc(C(=O)c2cccs2)c2sccc2n1 1\n",
      "Cc1noc(-c2sc(NC(=O)Cc3ccccc3)nc2-c2ccccc2)n1 1\n",
      "O=C(Cc1ccccc1)Nc1nc2nn(CCc3ccccc3)cc2c2nc(-c3ccccc3)nn12 0\n",
      "Nc1nc2ccccc2c2cn(-c3ccccc3)nc12 1\n",
      "COc1ccccc1CNC(=O)c1cc(-c2ccco2)nc(N)n1 1\n",
      "Cn1cc2c(nc(N)n3nc(-c4ccco4)nc23)n1 1\n",
      "CCCCn1cc2c(nc(NC(=O)Cc3ccccc3)n3nc(-c4ccco4)nc23)n1 0\n",
      "COc1ccc(CC(=O)Nc2cc(-n3nc(C)cc3C)nc(-c3ccc(C)o3)n2)cc1 1\n",
      "N#Cc1c(N)nc(-c2ccccc2)nc1-c1ccccc1 1\n",
      "Nc1nc(-c2ccco2)c2ncn(Cc3cccc4ccccc34)c2n1 1\n",
      "Nc1nc(-c2ccco2)nc2cn(Cc3ccccc3)nc12 1\n",
      "Cc1nc(C(=O)NCc2ccccn2)cc(-c2ccc(C)o2)n1 0\n",
      "CC(=O)Nc1cc(-c2ccccc2)nc(-n2nc(C)cc2C)n1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccccc4)cc3)cc2n(CCC)c1=O 1\n",
      "Nc1cc(-n2cccn2)nc(-c2ccco2)n1 1\n",
      "CN(C)c1nc(-c2cccs2)c2sccc2n1 1\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2ccccn2)n1)C1CC1 0\n",
      "Nc1nc(NCc2ccccc2)nc2nc(-c3ccco3)nn12 1\n",
      "Cn1cc2c(nc(NC(=O)Nc3ccco3)n3nc(-c4ccco4)nc23)n1 0\n",
      "O=C(Nc1cnc(-c2ccncc2)c(-c2ccncc2)n1)C1CC1 1\n",
      "Nc1nc(-c2ccco2)nc2sc(CN3CCC(O)CC3)cc12 1\n",
      "N#Cc1cccc(-c2nc(N)c3cc(Cc4ccccc4)sc3n2)c1 1\n",
      "CCOC(=O)c1cnc(NCc2ccccc2)n2nc(-c3ccco3)nc12 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(OCc5ccccc5)cc4)cc3)cc2n(CCC)c1=O 1\n",
      "Nc1nc(-c2ccccc2)nc2nn(Cc3ccccc3)cc12 1\n",
      "Cn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(CCN)cc4)cc3)cc2n(C)c1=O 1\n",
      "CCOC(=O)c1cnc(NCCOC)n2nc(-c3ccco3)nc12 1\n",
      "Nc1nc2nn(CCc3ccccc3)cc2c2nc(-c3ccccc3)nn12 1\n",
      "COc1cccc(Cn2nnc3c(-c4ccco4)nc(N)nc32)c1 1\n",
      "CCOC(=O)c1cnc(NC)n2nc(-c3ccco3)nc12 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(F)cc4)cc3)cc2n(CCC)c1=O 1\n",
      "Nc1nc2c(cnn2CCN2CCN(c3ccccc3)CC2)c2nc(-c3ccco3)nn12 1\n",
      "COc1ccc(NC(=O)Nc2nc3cnn(C)c3c3nc(-c4ccco4)nn23)cc1 1\n",
      "Cc1ccc(NC(=O)Nc2nc3nn(C)cc3c3nc(-c4ccco4)nn23)s1 0\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCC(c5ccccc5)CC4)cc3)cc2n(CCC)c1=O 0\n",
      "Nc1nc(C(=O)NCc2cccc3ccccc23)c2cccc(F)c2n1 1\n",
      "Nc1nc(-c2ccco2)c2ncn(Cc3ccccc3)c2n1 1\n",
      "CCNc1nc(-c2cccs2)c2sccc2n1 1\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2cccnc2)n1)C1CC1 0\n",
      "Nc1nc(N2CCN(Cc3ccccc3)CC2)nc2nc(-c3ccco3)nn12 1\n",
      "Cn1cc2c(nc(NC(=O)Cc3cccs3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc(C(=O)NCc2cccc3cccnc23)c2cccc(F)c2n1 1\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3ccccc3[N+](=O)[O-])c2n1 1\n",
      "Nc1nc(-c2cccc(C(F)(F)F)c2)nc2sc(Cc3ccccc3)cc12 0\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3ccco3)c2n1 1\n",
      "Nc1nc(-c2ccco2)nc2sc(CN3CCC(F)CC3)cc12 1\n",
      "CCCCc1nc2nc[nH]c2c2nc(-c3ccccc3)nn12 1\n",
      "CCN(CC)CCN(C)C(=O)c1ccc2c(c1)-c1nc(N)nc(-c3ccccc3)c1C2=O 1\n",
      "Nc1nc(C(=O)c2cccs2)c2sccc2n1 1\n",
      "Cn1cc2c(nc(NC(=O)Nc3ccccc3)n3nc(-c4ccco4)nc23)n1 0\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2ccc(F)cc2)n1)C1CC1 1\n",
      "Nc1nc(-c2ccco2)c2ncn(-c3ccccc3)c2n1 1\n",
      "CCc1nc2c(-c3ccccc3)cc(-c3ccccc3)nc2[nH]1 1\n",
      "Cn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccccc4)cc3)cc2n(C)c1=O 0\n",
      "Nc1nc(C(=O)NCc2ccccc2)c2cccc(F)c2n1 1\n",
      "Nc1cc(C(=O)N2CCCCC2)cc2nc(-c3ccco3)nn12 1\n",
      "Cc1cccc(Cn2cnc3c(-c4ccco4)nc(N)nc32)c1 1\n",
      "Cc1cccc(Cn2nnc3c(-c4ccco4)nc(N)nc32)c1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(C#N)cc4)cc3)nc2n(CCC)c1=O 0\n",
      "CCCn1cc2c(nc(NC(=O)Nc3ccc(F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Cc1ccc(-c2cc(C(=O)NCc3ccccn3)nc(N)n2)cc1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccccn4)cc3)cc2n(CCC)c1=O 1\n",
      "Cn1cc2c(nc(Cl)n3nc(-c4ccco4)nc23)n1 0\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccccc4)cc3)nc2n(CCC)c1=O 1\n",
      "Nc1nc(-n2cccn2)nc(-n2cccn2)c1Br 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(F)cc4)cc3)nc2n(CCC)c1=O 1\n",
      "CCNc1ncc(C(=O)OCC)c2nc(-c3ccco3)nn12 1\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3ccccc3F)c2n1 1\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2ccco2)n1)C1CC1 1\n",
      "Nc1nc2ccccc2n2c(=O)n(-c3ccccc3)nc12 1\n",
      "Cc1cccc(-n2nc3c(N)nc4ccccc4n3c2=O)c1 1\n",
      "N#Cc1cccc(-c2nc(N)c3cc(Cc4ccccc4)sc3n2)n1 1\n",
      "CCCn1cc2c(nc(NC(=O)Cc3ccccc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc(-c2ccccc2)cc2nc(-c3ccco3)nn12 1\n",
      "Nc1nc2c(cnn2CCc2ccccc2)c2nc(-c3ccco3)nn12 1\n",
      "COc1cccc(-c2cc(C(=O)NCc3ncccc3C)nc(N)n2)c1 1\n",
      "Nc1nc(N2CCNCC2)nc2nc(-c3ccco3)nn12 1\n",
      "N#Cc1c(-c2cccs2)cc(-c2ccccc2)nc1N 1\n",
      "CN1CCN(C(=O)c2ccc3c(c2)-c2nc(N)nc(-c4ccccc4)c2C3=O)CC1 1\n",
      "Nc1nc2c(cnn2CCc2ccc(O)cc2)c2nc(-c3ccco3)nn12 1\n",
      "Cn1cc2c(nc(NC(=O)Cc3ccc(F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "CCCCCc1nc2nc[nH]c2c2nc(-c3ccccc3)nn12 1\n",
      "Nc1nc(-c2ccccc2)nc2sc(-c3ccccc3)cc12 1\n",
      "CCn1cc2c(nc(N)n3nc(-c4ccco4)nc23)n1 1\n",
      "Cn1c(=O)c2[nH]c(-c3ccc(OCC(=O)NCc4ccccc4)cc3)cc2n(C)c1=O 1\n",
      "CCCn1cc2c(nc(N)n3nc(-c4ccco4)nc23)n1 1\n",
      "COc1ccc(CN(C)c2cc3nc(-c4ccco4)nn3c(N)n2)cc1 1\n",
      "CC(=O)Nc1cc(-c2cccs2)nc(-c2cccs2)n1 1\n",
      "COc1cccc2c1nc(N)n1nc(CN3CCN(c4cccc(F)c4)CC3C)nc21 1\n",
      "CCOC(=O)c1cnc(NC2CCCC2)n2nc(-c3ccco3)nc12 1\n",
      "COC(=O)c1cccc(Cn2cnc3c(-c4ccco4)nc(N)nc32)c1 1\n",
      "COC(=O)c1cccc(Cn2nnc3c(-c4ccco4)nc(N)nc32)c1 1\n",
      "Nc1nc(-c2ccccc2)c2c(n1)-c1cc(N3CCOCC3)ccc1C2=O 1\n",
      "CCCCn1cc2c(nc(NC(=O)Nc3ccc(F)cc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "N#Cc1c(-c2ccccc2)cc(-c2cccs2)nc1N 1\n",
      "CCCn1cc2c(nc(NC(=O)Nc3ccccc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "CCOc1nc2nn(C)cc2c2nc(-c3ccco3)nn12 0\n",
      "Nc1nc(-c2cccc(F)c2)nc2sc(Cc3ccccc3)cc12 1\n",
      "O=C(Nc1cnc(-c2ccncc2)c(-c2cnco2)n1)C1CC1 0\n",
      "CC(=O)Nc1cc(-c2cccc(O)c2)nc(-n2nc(C)cc2C)n1 1\n",
      "Cn1cc2c(nc(NC(=O)Nc3cccnc3)n3nc(-c4ccco4)nc23)n1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccccc4)cc3)cc2n(C)c1=O 1\n",
      "Cc1cc(C)n(-c2cc(NC(=O)CN3CCN(C)CC3)nc(-c3ccco3)n2)n1 1\n",
      "Nc1nc(-c2ccco2)nc2sc(CN3CCCC3)cc12 1\n",
      "CCCCn1cc2c(nc(NC(=O)Nc3cccc(OC)c3)n3nc(-c4ccco4)nc23)n1 1\n",
      "Cc1ccccc1-c1cc(C(=O)NCc2ccccn2)nc(N)n1 1\n",
      "COCCOc1ccc(N2CCN(CCn3ncc4c3nc(N)n3nc(-c5ccco5)nc43)CC2)cc1 1\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2ccncc2)n1)C1CC1 1\n",
      "CCn1cc2c(nc(NC(=O)Cc3ccccc3)n3nc(-c4ccco4)nc23)n1 0\n",
      "Nc1cc(C(=O)N2CCCC2)cn2nc(-c3ccco3)nc12 1\n",
      "O=C(Nc1cnc(-c2ccncc2)c(-c2cccnc2)n1)C1CC1 1\n",
      "O=C(Nc1cc(-c2ccccc2)nc(-c2ccccc2)n1)C1CC1 1\n",
      "Cn1cc2c(nc(NC(=O)Cc3cccc4ccccc34)n3nc(-c4ccco4)nc23)n1 1\n",
      "COc1cccc(Cn2cnc3c(-c4ccco4)nc(N)nc32)c1 1\n",
      "Cc1cc(C)n(-c2cc(NC(=O)CN3CCCC3)nc(-c3ccc(C)o3)n2)n1 1\n",
      "CCC(=O)Nc1nc(-c2ccccc2)cc(-c2ccccc2)n1 0\n",
      "Nc1nc2c(cnn2CCc2ccc(-c3ccccc3)cc2)c2nc(-c3ccco3)nn12 1\n",
      "Nc1nc(C(=O)NCc2ccccn2)cc(-c2ccccc2)n1 1\n",
      "Cn1cc2c(nc(NC(=O)Cc3ccccc3)n3nc(-c4ccco4)nc23)n1 0\n",
      "CCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCN(c5ccccc5)CC4)cc3)cc2n(CC)c1=O 0\n",
      "N#Cc1c(-c2ccccc2)cc(-c2ccccc2)nc1N 1\n",
      "CCc1nc(-c2ncc[nH]2)c2sccc2n1 1\n",
      "Nc1nc(-c2ccco2)c2nnn(Cc3cccc(C(F)(F)F)c3)c2n1 1\n",
      "COc1ccc(CCCn2ncc3c2nc(N)n2nc(-c4ccco4)nc32)cc1 1\n",
      "CC(=O)Nc1cc(-c2cc(C)cc(C)c2)nc(-n2nc(C)cc2C)n1 1\n",
      "Nc1nc(Cc2ccccc2)nc2cn(-c3ccccc3)nc12 1\n",
      "Nc1nc2ccccc2n2c(=O)c(-c3ccccc3)nn12 1\n",
      "CCCn1cc2c(nc(NC(=O)Cc3cccc4ccccc34)n3nc(-c4ccco4)nc23)n1 1\n",
      "Nc1nc(-c2cccs2)nc2sc(CN3CCCC3)cc12 1\n",
      "CCCCc1nc2nc[nH]c2c2nc(-c3ccc(C)cc3)nn12 1\n",
      "CCCCc1nc2nc[nH]c2c2nc(-c3ccco3)nn12 1\n",
      "O=C(O)c1cnc(NC2CC2)n2nc(-c3ccco3)nc12 0\n",
      "O=C(Nc1ccc(-c2ccncc2)c(-c2cnco2)n1)C1CC1 0\n",
      "CC(=O)Nc1cc(-c2ccco2)nc(-c2ccco2)n1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O=C(Nc1cc(-c2ccccc2)nc(-c2ccccc2)n1)C1CCCC1 1\n",
      "CCCCc1nc2n[nH]cc2c2nc(-c3ccccc3)nn12 1\n",
      "COc1ccccc1-c1cc(C(=O)NCc2ncccc2C)nc(N)n1 1\n",
      "CCCn1c(=O)c2[nH]c(-c3ccc(OCC(=O)Nc4ccc(Cl)cc4)cc3)cc2n(CCC)c1=O 1\n",
      "O=C(Nc1ccccc1)Nc1nc(NC2CCCCC2)nc2nc(-c3ccco3)nn12 1\n",
      "Uniqueness ratio: 0.79\n",
      "Ratio of new: 0.97\n"
     ]
    }
   ],
   "source": [
    "## But how many are there in the training set?\n",
    "good_smiles_tough = list(Chem.MolToSmiles(x) for x in unique_mols)\n",
    "count = 0\n",
    "new = 0\n",
    "recovered = 0\n",
    "for i, smi in enumerate(good_smiles_tough):\n",
    "    count += 1\n",
    "    if smi in init_data.SMILES.tolist():\n",
    "        print(smi, preds_tough[i])\n",
    "    else:\n",
    "        new += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = preds_tough == 1\n",
    "sel_mols_tough = np.asarray(good_mols_tough)[idx]\n",
    "sel_preds_tough = np.asarray(preds_tough)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'SMILES': good_smiles_tough, 'label': preds_tough})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(data_dir, 'ACGAN_A2AR_{}_epochs.csv'.format(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
